{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../src\")\n",
    "from utils import select_model, get_dataset, low_density_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56a811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_path = \"../results/epoch_budget/DTEC_DSIL_deterministic_0.5_s0_T400_bins7/18_Ionosphere\"\n",
    "experiment_config = os.path.join(experiment_path, \"experiment_config.yaml\")\n",
    "\n",
    "from hydra import initialize, compose\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_dataset_from_path(experiment_path, X):\n",
    "    cfg_experiment_path = Path(f\"{experiment_path}/experiment_config.yaml\")\n",
    "    model_path = f\"{experiment_path}/model.pth\"\n",
    "\n",
    "    with initialize(config_path=str(experiment_path), version_base=None):\n",
    "        cfg = compose(config_name=cfg_experiment_path.name)\n",
    "\n",
    "    model = select_model(cfg.model, device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.load_model(model_path, X)\n",
    "    return model, cfg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(experiment_path):\n",
    "    cfg_experiment_path = Path(f\"{experiment_path}/experiment_config.yaml\")\n",
    "    # Load as yaml\n",
    "    with open(cfg_experiment_path, 'r') as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "        if 'dataset_path' in config['dataset'].keys():\n",
    "            config['dataset']['dataset_path']   = \"../\" +  config['dataset']['dataset_path']\n",
    "        else:   \n",
    "            config['dataset']['dataset_path']   = \"../\" +  config['dataset']['data_path']\n",
    "\n",
    "    with initialize(config_path=str(experiment_path), version_base=None):\n",
    "        cfg = compose(config_name=cfg_experiment_path.name)\n",
    "    dataset = get_dataset(cfg)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a9ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(experiment_path)\n",
    "model, cfg = load_model_and_dataset_from_path(experiment_path, dataset['X_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9bca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get anomaly score of every samples\n",
    "def get_anomaly_score(model, X):\n",
    "    model.model.eval()\n",
    "    with torch.no_grad():\n",
    "        X = torch.from_numpy(X).float()\n",
    "        X = X.to(model.device)\n",
    "        anomaly_score = model.predict_score(X)\n",
    "    return anomaly_score\n",
    "\n",
    "anomaly_scores = get_anomaly_score(model, dataset['X_test'])\n",
    "y_pred = low_density_anomalies(-anomaly_scores, num_anomalies=dataset['y_test'].sum())\n",
    " # Get intersection between prediction and ground truth, both are 1 array of 0 and 1\n",
    "true_positive_indices = (y_pred == 1) & (dataset['y_test'] == 1)\n",
    "data_to_explain = dataset['X_test'][true_positive_indices]\n",
    "data_to_explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6860c461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from adbench.myutils import Utils\n",
    "utils = Utils()\n",
    "f1_score = f1_score(dataset['y_test'], y_pred)\n",
    "result = utils.metric(y_true=dataset['y_test'], y_score=anomaly_scores)\n",
    "print(f'AUCROC: {result[\"aucroc\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31131d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_explain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1c5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shap import KernelExplainer\n",
    "import shap\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf1edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take on random sample from the dataset\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "explainer= KernelExplainer(model.predict_score, data=np.zeros((1, dataset['X_test'].shape[1])), silent=True)\n",
    "shap_expl = explainer.shap_values(data_to_explain, nsamples=50, show_progress=False, silent=True)\n",
    "\n",
    "step=20\n",
    "w_explanations = np.array(model.instance_explanation(data_to_explain, agg=\"weighted_mean\", step=step))\n",
    "mean_explanations = np.array(model.instance_explanation(data_to_explain, agg=\"mean\", step=step))\n",
    "max_explanations  = np.array(model.instance_explanation(data_to_explain, agg=\"max\", step=step))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_expl = np.array(shap_expl)\n",
    "shap_expl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea4392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infidelity(x, model, attribution, num_samples=50, delta_std=0.01):\n",
    "    x= x.reshape(1, -1)\n",
    "    attribution = attribution.reshape(1, -1)\n",
    "    # Scale so that the sum is 1\n",
    "    attribution = attribution / np.sum(attribution)\n",
    "    n_features = x.shape[1]\n",
    "    f_x = model.predict_score(x)\n",
    "    infidelities = []\n",
    "    for _ in range(num_samples):\n",
    "        delta = np.random.normal(0, delta_std, size=(1, n_features))\n",
    "        f_x_delta = model.predict_score(x - delta)\n",
    "        dot = np.dot(delta.T, attribution)\n",
    "        error = (dot - f_x + f_x_delta)**2\n",
    "        infidelities.append(error)\n",
    "    return np.mean(infidelities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1459e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_to_explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc60f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_our_list = []\n",
    "inf_shap_list = []\n",
    "inf_mean_list = []\n",
    "inf_max_list = []\n",
    "for i in tqdm(range(len(data_to_explain))):\n",
    "    x_i = data_to_explain[i,:]\n",
    "    inf_our_list.append(infidelity(x_i, model, w_explanations[i], num_samples=100, delta_std=0.01))\n",
    "    inf_shap_list.append(infidelity(x_i, model, shap_expl[i], num_samples=100, delta_std=0.01))\n",
    "    inf_mean_list.append(infidelity(x_i, model, mean_explanations[i], num_samples=100, delta_std=0.01))\n",
    "    inf_max_list.append(infidelity(x_i, model, max_explanations[i], num_samples=100, delta_std=0.01))\n",
    "inf_our_list = np.array(inf_our_list)\n",
    "inf_shap_list = np.array(inf_shap_list)\n",
    "inf_mean_list = np.array(inf_mean_list)\n",
    "inf_max_list = np.array(inf_max_list)\n",
    "inf_our_list.mean(), inf_max_list.mean(), inf_mean_list.mean(), inf_shap_list.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc33662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "dataset_name = experiment_path.split(\"/\")[-1]\n",
    "inf_our_list = pd.DataFrame(inf_our_list, columns=[\"inf_our\"])\n",
    "inf_shap_list = pd.DataFrame(inf_shap_list, columns=[\"inf_shap\"])\n",
    "inf_mean_list = pd.DataFrame(inf_mean_list, columns=[\"inf_mean\"])\n",
    "inf_max_list = pd.DataFrame(inf_max_list, columns=[\"inf_max\"])\n",
    "inf_list = pd.concat([inf_our_list, inf_shap_list, inf_mean_list, inf_max_list], axis=1)\n",
    "inf_list.to_csv(os.path.join(f\"infidelity_{dataset_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_score(data_to_explain[0,:].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def faithfulness(f, g, x, x_baseline, subset_size=5, num_samples=100):\n",
    "    d = x.shape[0]\n",
    "    attributions = g(x)  # Vector of shape (d,)\n",
    "\n",
    "    attr_sums = []\n",
    "    output_diffs = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Randomly select a subset of feature indices\n",
    "        S = np.random.choice(d, size=subset_size, replace=False)\n",
    "\n",
    "        # Attribution sum over S\n",
    "        sum_attr = np.sum(attributions[S])\n",
    "\n",
    "        # Create x with features in S replaced by baseline values\n",
    "        x_masked = x.copy()\n",
    "        x_masked[S] = x_baseline[S]\n",
    "\n",
    "        # Difference in model output\n",
    "        delta_f = f(x.reshape(1, -1)) - f(x_masked.reshape(1, -1))\n",
    "\n",
    "        attr_sums.append(sum_attr)\n",
    "        output_diffs.append(delta_f)\n",
    "    attr_sums = np.array(attr_sums)\n",
    "    output_diffs = np.array(output_diffs)\n",
    "    # Compute Pearson correlation between attribution sums and output differences\n",
    "    corr, _ = pearsonr(attr_sums, output_diffs.squeeze())\n",
    "    # Replace NaN with 0\n",
    "    if np.isnan(corr):\n",
    "        corr = 0\n",
    "    return corr\n",
    "\n",
    "x_baseline = np.zeros((dataset['X_test'].shape[1]))\n",
    "\n",
    "import pandas as pd\n",
    "faithfulness_df = pd.DataFrame()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"shap\")\n",
    "for k in range(3,dataset['X_train'].shape[1], 2):\n",
    "    result = {}\n",
    "    for explanation_method in ['shap' , 'mean', 'max', 'our']:\n",
    "        if explanation_method == 'shap':\n",
    "            g = lambda x: explainer.shap_values(x, nsamples=50, show_progress=False)\n",
    "        elif explanation_method == 'mean':\n",
    "            g = lambda x: model.instance_explanation(x, agg=\"mean\", step=10)\n",
    "        elif explanation_method == 'max':\n",
    "            g = lambda x: model.instance_explanation(x, agg=\"max\", step=10)\n",
    "        elif explanation_method == 'our':\n",
    "            g = lambda x: model.instance_explanation(x, agg=\"weighted_mean\", step=10)\n",
    "        faithfulness_list = []\n",
    "        for idx in tqdm(range(len(data_to_explain))):\n",
    "            x_i = data_to_explain[idx,:]\n",
    "            faithfulness_list.append(faithfulness(model.predict_score, g, x_i, x_baseline, subset_size=k, num_samples=100))\n",
    "        faithfulness_list = np.array(faithfulness_list)\n",
    "        result[explanation_method] = faithfulness_list.mean()\n",
    "    faithfulness_df = pd.concat([faithfulness_df, pd.DataFrame(result, index=[k])])\n",
    "    print(f'k: {k}, {result}')\n",
    "faithfulness_df = faithfulness_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a5094",
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_df.to_csv(os.path.join(f\"faithfulness_{dataset_name}.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1755be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "def compute_fidelity_regression(x, f, g, train_set, top_k=5, n_samples=100, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    idx = rng.choice(len(train_set), size=n_samples, replace=False)\n",
    "    X_subset = train_set[idx]\n",
    "    x = x.reshape(1, -1)\n",
    "\n",
    "    importances = g(x).reshape(1,-1) # shape: (n_samples, n_features)\n",
    "    important_indices = np.argsort(-np.abs(importances), axis=1)[:, :top_k]  # shape: (n_samples, top_k)\n",
    "    score_orig = f(x)\n",
    "\n",
    "    # Create the perturbed dataset\n",
    "    X_subset[:, important_indices] = x[:,important_indices]\n",
    "    score_perturbed = f(X_subset) \n",
    "    score_orig = score_orig.repeat(n_samples, axis=0)\n",
    "    \n",
    "\n",
    "    fidelity_score = mean_squared_error(score_orig, score_perturbed)\n",
    "    return fidelity_score\n",
    "fidelity_result_pd = pd.DataFrame()\n",
    "for k in range(3,dataset['X_train'].shape[1], 2):\n",
    "    fidelity_result = {\"k\": k}\n",
    "    for explanation_method in [\"shap\", 'mean', 'max', 'our']:\n",
    "        if explanation_method == 'shap':\n",
    "            g = lambda x: explainer.shap_values(x, nsamples=50, show_progress=False,silent=True)\n",
    "        elif explanation_method == 'mean':\n",
    "            g = lambda x: model.instance_explanation(x, agg=\"mean\", step=10)\n",
    "        elif explanation_method == 'max':\n",
    "            g = lambda x: model.instance_explanation(x, agg=\"max\", step=10)\n",
    "        elif explanation_method == 'our':\n",
    "            g = lambda x: model.instance_explanation(x, agg=\"weighted_mean\", step=10)\n",
    "        fidelity_list = []\n",
    "        for idx in tqdm(range(len(data_to_explain))):\n",
    "            x_i = data_to_explain[idx,:]\n",
    "            fidelity_list.append(compute_fidelity_regression(x_i, model.predict_score, g, train_set=dataset['X_train'], top_k=k, n_samples=100))\n",
    "        fidelity_list = np.array(fidelity_list)\n",
    "        fidelity_result[explanation_method] = fidelity_list.mean()\n",
    "    print(f'{fidelity_result}')\n",
    "    fidelity_result_pd = pd.concat([fidelity_result_pd, pd.DataFrame(fidelity_result, index=[0])], axis=0)\n",
    "fidelity_result_pd = fidelity_result_pd.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e4abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fidelity_result_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b58de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef8e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything, infidelity, faithfulness and fidelity\n",
    "\n",
    "fidelity_result_pd.to_csv(os.path.join(f\"fidelity_{dataset_name}.csv\"), index=False)\n",
    "\n",
    "print(f\"Saved infidelity, faithfulness and fidelity to {dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e4906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f204e50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c85a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a652a4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce159c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c230e3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dac1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa46c6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d6e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dbb4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4665b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37257e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee815cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec313ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07fddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
