{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from utils import (check_cuda, get_dataset, low_density_anomalies,\n",
    "                   select_model, setup_experiment)\n",
    "from utils import get_dataset, select_model\n",
    "from hydra import initialize, compose\n",
    "from shap_explainer import ShapExplainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_dataset_from_path(experiment_path):\n",
    "    cfg_experiment_path = Path(f\"{experiment_path}/experiment_config.yaml\")\n",
    "    model_path = f\"{experiment_path}/model.pth\"\n",
    "        \n",
    "    with initialize(config_path=str(experiment_path), version_base=None):\n",
    "        cfg = compose(config_name=cfg_experiment_path.name)\n",
    "    cfg.dataset.dataset_path = \"../\" + cfg.dataset.dataset_path\n",
    "    dataset = get_dataset(cfg)\n",
    "    X = dataset['X_test']\n",
    "    y = dataset['y_test']\n",
    "    explanation = dataset['explanation_test']\n",
    "\n",
    "    # keep only data with label != 0\n",
    "    X = X[y != 0]\n",
    "    explanation = explanation[y != 0]\n",
    "    y = y[y != 0]\n",
    "\n",
    "    model = select_model(cfg.model, device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.load_model(model_path, X)\n",
    "    return model, dataset, cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explanation_accuracy(explanation,ground_truth, k=\"auto\"):\n",
    "    if explanation.shape != ground_truth.shape:\n",
    "        raise ValueError(\n",
    "            \"The explanation and ground truth must have the same shape.\"\n",
    "        )\n",
    "    if len(explanation.shape) == 1:\n",
    "        explanation = explanation.reshape(1, -1)\n",
    "    if len(ground_truth.shape) == 1:\n",
    "        ground_truth = ground_truth.reshape(1, -1)\n",
    "    if type(explanation) is torch.Tensor:\n",
    "        explanation = explanation.cpu().detach().numpy()\n",
    "    if type(ground_truth) is torch.Tensor:\n",
    "        ground_truth = ground_truth.cpu().detach().numpy()\n",
    "    accuracy = []\n",
    "    for row in range(ground_truth.shape[0]):\n",
    "        if k == \"auto\":\n",
    "            k_ = int(np.sum(ground_truth[row]))\n",
    "        else:\n",
    "            k_ = k\n",
    "        if k_ == 0 or int(np.sum(ground_truth[row])) == 0:\n",
    "            continue\n",
    "        sorted_indices = np.argsort(explanation[row])[::-1]\n",
    "        instance_explanation = np.zeros_like(explanation[row])\n",
    "        instance_explanation[sorted_indices[:k_]] = 1\n",
    "\n",
    "        instance_accuracy = (\n",
    "            np.sum(ground_truth[row] * instance_explanation) / k_\n",
    "        )\n",
    "        accuracy.append(instance_accuracy)\n",
    "    return np.mean(accuracy)\n",
    "\n",
    "\n",
    "def dcg_score_matrix_p(importance_scores, relevance_matrix, p):\n",
    "    \"\"\"\n",
    "    Compute the DCG scores at a given cutoff rank p.\n",
    "    \"\"\"\n",
    "    importance_scores = np.array(importance_scores)\n",
    "    relevance_matrix = np.array(relevance_matrix)\n",
    "    importance_scores = importance_scores.squeeze()\n",
    "    relevance_matrix = relevance_matrix.squeeze()\n",
    "    assert (\n",
    "        importance_scores.shape == relevance_matrix.shape\n",
    "    ), \"importance_scores and relevance_matrix must have the same shape\"\n",
    "\n",
    "    # Sort relevance based on importance scores\n",
    "    if len(importance_scores.shape) == 1:\n",
    "        importance_scores = importance_scores.reshape(1, -1)\n",
    "        relevance_matrix = relevance_matrix.reshape(1, -1)\n",
    "\n",
    "    sorted_indices = np.argsort(importance_scores, axis=1)[:, ::-1]\n",
    "    sorted_relevance = np.take_along_axis(\n",
    "        relevance_matrix, sorted_indices, axis=1\n",
    "    )\n",
    "\n",
    "    # Consider only the top p items\n",
    "    sorted_relevance_p = sorted_relevance[:, :p]\n",
    "    ranks = np.arange(1, p + 1)  # Ranks from 1 to p\n",
    "\n",
    "    # Compute DCG scores\n",
    "    dcg_scores = np.sum(sorted_relevance_p / np.log2(ranks + 1), axis=1)\n",
    "\n",
    "    return dcg_scores\n",
    "\n",
    "\n",
    "def idcg_score_matrix_p(relevance_matrix, p):\n",
    "    \"\"\"\n",
    "    Compute the IDCG scores at a given cutoff rank p.\n",
    "    \"\"\"\n",
    "    if len(relevance_matrix.shape) == 1:\n",
    "        relevance_matrix = relevance_matrix.reshape(1, -1)\n",
    "    relevance_matrix = np.array(relevance_matrix)\n",
    "    sorted_relevance = np.sort(relevance_matrix, axis=1)[:, ::-1]\n",
    "\n",
    "    # Consider only the top p items\n",
    "    sorted_relevance_p = sorted_relevance[:, :p]\n",
    "    ranks = np.arange(1, p + 1)  # Ranks from 1 to p\n",
    "\n",
    "    # Compute IDCG scores\n",
    "    idcg_scores = np.sum(sorted_relevance_p / np.log2(ranks + 1), axis=1)\n",
    "\n",
    "    return idcg_scores\n",
    "\n",
    "\n",
    "def nDCG_(importance_scores, relevance_matrix, p):\n",
    "    \"\"\"\n",
    "    Compute the nDCG scores at a given cutoff rank p.\n",
    "    \"\"\"\n",
    "    dcg_scores_p = dcg_score_matrix_p(importance_scores, relevance_matrix, p)\n",
    "    idcg_scores_p = idcg_score_matrix_p(relevance_matrix, p)\n",
    "\n",
    "    # Compute normalized DCG\n",
    "    ndcg_scores_p = np.zeros_like(dcg_scores_p)\n",
    "    for i in range(len(dcg_scores_p)):\n",
    "        if idcg_scores_p[i] == 0:\n",
    "            ndcg_scores_p[i] = 0\n",
    "        else:\n",
    "            ndcg_scores_p[i] = dcg_scores_p[i] / idcg_scores_p[i]\n",
    "\n",
    "    return ndcg_scores_p\n",
    "\n",
    "\n",
    "def nDCG_p(importance_scores, relevance_matrix, k= 'auto'):\n",
    "    nDCG_scores = []\n",
    "    if len(importance_scores.shape) == 1:\n",
    "        importance_scores = importance_scores.reshape(1, -1)\n",
    "    if len(relevance_matrix.shape) == 1:\n",
    "        relevance_matrix = relevance_matrix.reshape(1, -1)\n",
    "    for i in range(importance_scores.shape[0]):\n",
    "        if k == \"auto\":\n",
    "            k_ = int(np.sum(relevance_matrix[i]))\n",
    "        else:\n",
    "            k_ = k\n",
    "        if k_ == 0 or int(np.sum(relevance_matrix[i])) == 0:\n",
    "            continue\n",
    "        nDCG_scores.append(nDCG_(importance_scores[i], relevance_matrix[i], p=k_))\n",
    "    return np.array(nDCG_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def run_explanation(model, data, saving_path, cfg, diffusion_perturbation_step=50, shap_ratio=1.5):\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    score = model.predict_score(data[\"X_test\"], device=device).squeeze()\n",
    "    data[\"y_test\"][data[\"y_test\"] > 0] = 1\n",
    "    metric_df = pd.read_csv(Path(saving_path, \"model_metrics.csv\"))\n",
    "    indices = np.arange(len(data[\"y_test\"]))\n",
    "    # Suppose we know how much anomaly are in the dataset\n",
    "    y_pred = low_density_anomalies(-score, len(indices[data[\"y_test\"] == 1]))\n",
    "    pred_indices = np.arange(len(data[\"X_test\"]))[y_pred == 1]\n",
    "    samples = data[\"X_test\"][pred_indices]\n",
    "    expected_explanation = data[\"explanation_test\"][pred_indices]\n",
    "    truth_label = data[\"y_test\"][pred_indices]\n",
    "    # Keep only the samples for which turth_lab is not 0\n",
    "    samples = samples[truth_label != 0]\n",
    "    if len(samples) == 0:\n",
    "        print(\"No anomaly detected\")\n",
    "        return\n",
    "    expected_explanation = expected_explanation[truth_label != 0]\n",
    "    truth_label = truth_label[truth_label != 0]\n",
    "    existing_columns = metric_df.columns\n",
    "    force_rerun = True\n",
    "    print(f'Number of samples: {len(samples)}')\n",
    "    cfg.output_path = \"../\" + cfg.output_path\n",
    "    saving_path, experiment_name = setup_experiment(cfg)\n",
    "    # if 'gradient_accuracy' not in existing_columns or force_rerun:\n",
    "    #     print('Gradient...')\n",
    "    #     start = time.time()\n",
    "    #     explanation = model.gradient_explanation(samples)\n",
    "    #     end = time.time()\n",
    "    #     nDCG = nDCG_p(explanation, expected_explanation).mean()\n",
    "    #     accuracy = explanation_accuracy(explanation, expected_explanation).mean()\n",
    "    #     gradient_explanation_time = end - start\n",
    "    #     metric_df[\"gradient_accuracy\"] = accuracy\n",
    "    #     metric_df[\"gradient_ndcg\"] = nDCG\n",
    "    #     metric_df[\"gradient_time\"] = gradient_explanation_time\n",
    "    #     metric_df[\"mean_gradient_time\"] = gradient_explanation_time / len(samples)\n",
    "    #     np.save(Path(saving_path, f\"gradient_explanation.npy\"), explanation)\n",
    "    #     print(f'nDCG: {nDCG}, accuracy: {accuracy}, time: {gradient_explanation_time},dataset: {experiment_name}')\n",
    "    # if \"new_mean_diffusion_accuracy\" not in existing_columns or force_rerun:\n",
    "    #     print(\"Mean diffusion...\")\n",
    "    #     start = time.time()\n",
    "    #     explanation = model.instance_explanation(\n",
    "    #         samples, saving_path=saving_path, step=diffusion_perturbation_step, agg=\"mean\"\n",
    "    #     )\n",
    "    #     end = time.time()\n",
    "    #     nDCG = nDCG_p(explanation, expected_explanation).mean()\n",
    "    #     accuracy = explanation_accuracy(explanation, expected_explanation).mean()\n",
    "    #     mean_diffusion_explanation_time = end - start\n",
    "    #     metric_df[\"new_mean_diffusion_accuracy\"] = accuracy\n",
    "    #     metric_df[\"new_mean_diffusion_ndcg\"] = nDCG\n",
    "    #     metric_df[\"new_mean_diffusion_time\"] = mean_diffusion_explanation_time\n",
    "    #     metric_df[\"mean_new_mean_diffusion_time\"] = mean_diffusion_explanation_time / len(samples)\n",
    "    #     np.save(Path(saving_path, f\"new_mean_diffusion_explanation.npy\"), explanation)\n",
    "    #     print(f'nDCG: {nDCG}, accuracy: {accuracy}')\n",
    "    #     print(f'Mean computation time: {mean_diffusion_explanation_time / len(samples)}')\n",
    "    if \"new_max_diffusion_accuracy\" not in existing_columns or force_rerun:\n",
    "        print(\"Max diffusion...\")\n",
    "        start = time.time()\n",
    "        explanation = model.instance_explanation(\n",
    "            samples, saving_path=saving_path, step=diffusion_perturbation_step, agg=\"max\"\n",
    "        )\n",
    "        end = time.time()\n",
    "        nDCG = nDCG_p(explanation, expected_explanation).mean()\n",
    "        accuracy = explanation_accuracy(explanation, expected_explanation).mean()\n",
    "        max_diffusion_explanation_time = end - start\n",
    "        metric_df[\"new_max_diffusion_accuracy\"] = accuracy\n",
    "        metric_df[\"new_max_diffusion_ndcg\"] = nDCG\n",
    "        metric_df[\"new_max_diffusion_time\"] = max_diffusion_explanation_time\n",
    "        metric_df[\"mean_new_max_diffusion_time\"] = max_diffusion_explanation_time / len(samples)\n",
    "        np.save(Path(saving_path, f\"new_max_diffusion_explanation.npy\"), explanation)\n",
    "        print(f'nDCG: {nDCG}, accuracy: {accuracy}')\n",
    "        print(f'Mean computation time: {max_diffusion_explanation_time / len(samples)}')\n",
    "\n",
    "    # if \"adatative_shap_explanation_accuracy\" not in existing_columns or force_rerun:\n",
    "    #     print('SHAP...')\n",
    "    #     subset = np.random.choice(\n",
    "    #         np.arange(len(data[\"X_train\"])), size=int(0.02 * data['X_train'].shape[0]), replace=False\n",
    "    #     )\n",
    "    #     subset = data[\"X_train\"][subset]\n",
    "    #     n_samples = int(shap_ratio * data[\"X_test\"].shape[1])\n",
    "    #     print()\n",
    "    #     start = time.time()\n",
    "    #     shap_explainer = ShapExplainer(model, subset)\n",
    "    #     explanation = shap_explainer.explain_instance(\n",
    "    #         samples,\n",
    "    #         nsamples=n_samples,\n",
    "    #     ).squeeze()\n",
    "    #     end = time.time()\n",
    "    #     shap_nDCG = nDCG_p(explanation, expected_explanation).mean()\n",
    "    #     shap_accuracy = explanation_accuracy(explanation, expected_explanation).mean()\n",
    "    #     shap_explanation_time = end - start\n",
    "    #     metric_df['adaptative_shap_accuracy'] = shap_accuracy\n",
    "    #     metric_df['adaptative_shap_ndcg'] = shap_nDCG\n",
    "    #     metric_df['adaptative_shap_time'] = shap_explanation_time\n",
    "    #     metric_df['mean_adaptative_shap_time'] = shap_explanation_time / len(samples)\n",
    "    #     np.save(Path(saving_path, f\"new_shap_diffusion_explanation.npy\"), explanation)\n",
    "    #     print(f'nDCG: {shap_nDCG}, accuracy: {shap_accuracy}')\n",
    "    #     print(f'Mean computation time: {shap_explanation_time / len(samples)}')\n",
    "    # Save metrics df\n",
    "    metric_df.to_csv(Path(saving_path, \"model_metrics.csv\"))\n",
    "    np.save(Path(saving_path, f\"expected_explanation.npy\"), expected_explanation)\n",
    "    print(f'Sved at {saving_path}')\n",
    "    return metric_df[\"new_max_diffusion_ndcg\"], metric_df[\"new_max_diffusion_accuracy\"], metric_df[\"new_max_diffusion_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_each_dataset_version(experiment_path, seeds=[], diffusion_perturbation_step=50, shap_r=1.5):\n",
    "        acc_lst = []\n",
    "        ndcg_lst = []\n",
    "        time_lst = []\n",
    "        for seed in seeds:\n",
    "            print(f\"Processing {experiment_path} with seed {seed}\")\n",
    "            seed_experiment_path = f\"{experiment_path}_seed_{seed}\"\n",
    "            with open(f\"{seed_experiment_path}/experiment_config.yaml\", \"r\") as f:\n",
    "                config = yaml.safe_load(f)\n",
    "                if \"dataset_path\" not in config[\"dataset\"]:\n",
    "                    config[\"dataset\"][\"dataset_path\"] = config[\"dataset\"][\"data_path\"]\n",
    "            model, dataset, cfg = load_model_and_dataset_from_path(seed_experiment_path)\n",
    "            cfg.dataset.dataset_path = config['dataset']['dataset_path']\n",
    "            ndcg, acc,time = run_explanation(model, dataset, seed_experiment_path, cfg, diffusion_perturbation_step, shap_r)\n",
    "            acc_lst.append(acc)\n",
    "            ndcg_lst.append(ndcg)\n",
    "            time_lst.append(time)\n",
    "        print(f\"###################################\")\n",
    "        print(f\"Mean accuracy: {np.mean(acc_lst)}\\pm{np.std(acc_lst)}, mean nDCG: {np.mean(ndcg_lst)}\\pm{np.std(ndcg_lst)}, mean time: {np.mean(time_lst)}\\pm{np.std(time_lst)}\")\n",
    "        return np.mean(acc_lst), np.mean(ndcg_lst), np.mean(time_lst), np.std(acc_lst), np.std(ndcg_lst), np.std(time_lst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_mode = [\"DSIL_deterministic_0.5\"]\n",
    "datasets_name = [\n",
    "     \"A_synthetic_f1000_s5000_c40_r0.01_0.01_0.01_0.01_0.01\",\n",
    "     \"A_synthetic_f1000_s5000_c40_r0.02_0.02_0.02_0.02_0.02\",\n",
    "]\n",
    "\n",
    "diffusion_perturbation_per_dataset = {\n",
    "}\n",
    "models= ['DTEC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for training in training_mode:\n",
    "    for model in models:\n",
    "        for dataset_name in datasets_name:\n",
    "            experiment_path = f\"../results/all_db_all_training/{model}_{training}_s0_T400_bins7/{dataset_name}\"\n",
    "            process_each_dataset_version(experiment_path, seeds=[0, 1, 2, 3, 4], diffusion_perturbation_step=2, shap_r=1.5)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
